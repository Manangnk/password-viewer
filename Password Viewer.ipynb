{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.imports import *\n",
    "from fastai.torch_imports import *\n",
    "from fastai.dataset import *\n",
    "from fastai.learner import *\n",
    "from fastai.metrics import *\n",
    "from fastai.core import *\n",
    "from fastai.model import *\n",
    "from fastai.lm_rnn import *\n",
    "from collections import *\n",
    "from fastai.conv_learner import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = torchvision.transforms.Normalize(\n",
    "   mean=[0.485, 0.456, 0.406],\n",
    "   std=[0.229, 0.224, 0.225]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = torchvision.transforms.Compose([\n",
    "   torchvision.transforms.Resize(256),\n",
    "   torchvision.transforms.CenterCrop(240),\n",
    "   torchvision.transforms.ToTensor(),\n",
    "   normalize\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PasswordDataset(Dataset):\n",
    "    def __init__(self, files, ys):\n",
    "        self.files = files\n",
    "        self.ys = ys\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return preprocess(PIL.Image.open(self.files[idx])).float().cuda(), self.ys[idx].long().cuda()\n",
    "    \n",
    "    def __len__(self): return len(self.ys)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_files(cls, img_path, y_path, cv_idxs):\n",
    "        files = np.array(glob(img_path))\n",
    "\n",
    "        with open(y_path) as f:\n",
    "            ys = torch.Tensor([int(x) for x in f.read().split(\"\\n\") if x != ''])\n",
    "        \n",
    "        print(ys[cv_idxs])\n",
    "        return cls(files[~cv_idxs], ys[~cv_idxs]), cls(files[cv_idxs], ys[cv_idxs])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = \"/home/robert/Documents/typingviewer/data/*.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_path = \"/home/robert/Documents/typingviewer/data/y.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = len(glob(image_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_idxs = get_cv_idxs(dataset_size, val_pct=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 3])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 7.,  9.])\n"
     ]
    }
   ],
   "source": [
    "trn_ds, val_ds = PasswordDataset.from_files(image_paths, y_path, cv_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "          [-2.1008, -2.1008, -2.1008,  ..., -2.1179, -2.1179, -2.1179],\n",
       "          ...,\n",
       "          [-1.9467, -1.9124, -1.9295,  ..., -1.9124, -1.8782, -1.9124],\n",
       "          [-1.9980, -1.9980, -1.9638,  ..., -2.0837, -2.1008, -2.1008],\n",
       "          [-2.0665, -2.0665, -2.0665,  ..., -1.9124, -1.9295, -1.9295]],\n",
       " \n",
       "         [[-1.3179, -1.3004, -1.3004,  ...,  0.3978,  0.4153,  0.4153],\n",
       "          [-1.3179, -1.3004, -1.3004,  ...,  0.4153,  0.4153,  0.4153],\n",
       "          [-1.3004, -1.3004, -1.3004,  ...,  0.4328,  0.4328,  0.4503],\n",
       "          ...,\n",
       "          [-1.7556, -1.7206, -1.7381,  ..., -1.6681, -1.6155, -1.5980],\n",
       "          [-1.8081, -1.8081, -1.7731,  ..., -1.6331, -1.6506, -1.6155],\n",
       "          [-1.9132, -1.9132, -1.8782,  ..., -1.1078, -1.1078, -1.0903]],\n",
       " \n",
       "         [[-0.8458, -0.8284, -0.8284,  ...,  0.7054,  0.7054,  0.6879],\n",
       "          [-0.8458, -0.8284, -0.8284,  ...,  0.7054,  0.7054,  0.6879],\n",
       "          [-0.8633, -0.8458, -0.8458,  ...,  0.7228,  0.7054,  0.6879],\n",
       "          ...,\n",
       "          [-1.5256, -1.4907, -1.5081,  ..., -1.4036, -1.3339, -1.3164],\n",
       "          [-1.5779, -1.5779, -1.5430,  ..., -1.4036, -1.4036, -1.3687],\n",
       "          [-1.6650, -1.6650, -1.6476,  ..., -0.9156, -0.9156, -0.8807]]], device='cuda:0'),\n",
       " tensor(4, device='cuda:0'))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_ds[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_dl = torch.utils.data.DataLoader(trn_ds, batch_size=bs)\n",
    "val_dl = torch.utils.data.DataLoader(val_ds, batch_size=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, out_dim=10):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=3))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=3))\n",
    "        self.fc = nn.Linear(26*26*32, out_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        #print(out.size())\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "myConv = SingleModel(ConvNet().cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "md = ModelData('model', trn_dl, val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def faux_loss(preds):\n",
    "    print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "myConv = SingleModel(ConvnetBuilder(resnet18, 200, False, False).model[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#myConv.model[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = Learner(md, myConv, opt_fn=optim.Adam, crit=F.nll_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f302279d53874a0b9ca5e2621188a25b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss       \n",
      "    0      7.914818   4.674003  \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[4.674002647399902]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.fit(1*1e-3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PasswordStatefulLSTM(nn.Module):\n",
    "    def __init__(self, bs):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.cnn_final_width = 200\n",
    "        self.number_rnn_layers = 1\n",
    "        self.rnn_hidden_count = 100\n",
    "        self.output_size = 10\n",
    "        \n",
    "        self.rnn = nn.LSTM(self.cnn_final_width, self.rnn_hidden_count, self.number_rnn_layers, dropout=0.5)\n",
    "        self.l_out = nn.Linear(self.rnn_hidden_count, self.output_size)\n",
    "        self.init_hidden(bs)\n",
    "        self.conv = ConvNet(out_dim=self.cnn_final_width)\n",
    "        \n",
    "    def forward(self, cs):\n",
    "        rnn_input = self.conv(cs).unsqueeze(1) #seq_len, batch, input_size\n",
    "        #bs = cs[0].size(0)\n",
    "        #if self.h[0].size(1) != bs: self.init_hidden(bs)\n",
    "        outp,h = self.rnn(rnn_input, self.h)\n",
    "        self.h = repackage_var(h)\n",
    "        return F.log_softmax(self.l_out(outp), dim=-1).view(-1, self.output_size)\n",
    "    \n",
    "    def init_hidden(self, bs):\n",
    "        #self.h = (V(torch.zeros(self.number_rnn_layers, bs, self.rnn_hidden_count)),\n",
    "        #          V(torch.zeros(self.number_rnn_layers, bs, self.rnn_hidden_count)))\n",
    "        self.h = (V(torch.zeros(1, 1, self.rnn_hidden_count)),\n",
    "                  V(torch.zeros(1, 1, self.rnn_hidden_count)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robert/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "myLSTM = SingleModel(PasswordStatefulLSTM(bs).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = Learner(md, myLSTM, opt_fn=optim.Adam, crit=F.nll_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f6413d58a894b1798633e970d51d4dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch'), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss       \n",
      "    0      2.248991   2.431012  \n",
      "    1      2.014701   2.901056       \n",
      "    2      1.812328   3.507516       \n",
      "    3      1.640727   4.049956       \n",
      "    4      1.498911   4.341283       \n",
      "    5      1.388798   4.55842        \n",
      "    6      1.304812   4.760144       \n",
      "    7      1.238718   4.896334       \n",
      "    8      1.185136   5.045734       \n",
      "    9      1.140917   5.184613       \n",
      "    10     1.10362    5.296159       \n",
      "    11     1.071793   5.345508       \n",
      "    12     1.044014   5.448779       \n",
      "    13     1.019801   5.541713       \n",
      "    14     0.998449   5.632837       \n",
      "    15     0.979462   5.729972       \n",
      "    16     0.962452   5.804691       \n",
      "    17     0.947132   5.865543       \n",
      "    18     0.933282   5.982412       \n",
      "    19     0.920613   6.069868       \n",
      "    20     0.909025   6.146122       \n",
      "    21     0.898389   6.219546       \n",
      "    22     0.888592   6.290355       \n",
      "    23     0.879538   6.358792       \n",
      "    24     0.871148   6.427435       \n",
      "    25     0.86335    6.535831       \n",
      "    26     0.856078   6.570115       \n",
      "    27     0.849268   6.631074       \n",
      "    28     0.842888   6.693925       \n",
      "    29     0.836917   6.746935       \n",
      "    30     0.83131    6.799399       \n",
      "    31     0.826035   6.849902       \n",
      "    32     0.821064   6.898266       \n",
      "    33     0.816374   6.944456       \n",
      "    34     0.811942   6.988495       \n",
      "    35     0.807747   7.030456       \n",
      "    36     0.803771   7.070496       \n",
      "    37     0.799999   7.109931       \n",
      "    38     0.796415   7.158654       \n",
      "    39     0.793025   7.196187       \n",
      "    40     0.789792   7.233888       \n",
      "    41     0.786703   7.271427       \n",
      "    42     0.783748   7.308323       \n",
      "    43     0.780924   7.34398        \n",
      "    44     0.778227   7.37776        \n",
      "    45     0.775652   7.409126       \n",
      "    46     0.773192   7.437807       \n",
      "    47     0.770836   7.463881       \n",
      "    48     0.768576   7.487733       \n",
      "    49     0.766405   7.509941       \n",
      "    50     0.764317   7.53115        \n",
      "    51     0.76231    7.551966       \n",
      "    52     0.760383   7.572865       \n",
      "    53     0.75853    7.594155       \n",
      "    54     0.756749   7.615957       \n",
      "    55     0.755035   7.63823        \n",
      "    56     0.753383   7.660805       \n",
      "    57     0.751789   7.683414       \n",
      "    58     0.750251   7.705722       \n",
      "    59     0.748766   7.727377       \n",
      "    60     0.747333   7.748066       \n",
      "    61     0.745951   7.767583       \n",
      "    62     0.744616   7.785865       \n",
      "    63     0.743325   7.803013       \n",
      "    64     0.742077   7.819255       \n",
      "    65     0.740869   7.834895       \n",
      "    66     0.739699   7.850255       \n",
      "    67     0.738567   7.865617       \n",
      "    68     0.737471   7.88118        \n",
      "    69     0.73641    7.897048       \n",
      "    70     0.735382   7.913218       \n",
      "    71     0.734385   7.929598       \n",
      "    72     0.733418   7.946029       \n",
      "    73     0.732479   7.962311       \n",
      "    74     0.731569   7.978246       \n",
      "    75     0.730686   7.993669       \n",
      "    76     0.729828   8.008484       \n",
      "    77     0.728996   8.022679       \n",
      "    78     0.728187   8.036327       \n",
      "    79     0.727401   8.049559       \n",
      "    80     0.726637   8.062548       \n",
      "    81     0.725895   8.075457       \n",
      "    82     0.725173   8.088423       \n",
      "    83     0.724471   8.101528       \n",
      "    84     0.723788   8.114795       \n",
      "    85     0.723124   8.128187       \n",
      "    86     0.722478   8.141623       \n",
      "    87     0.721848   8.154991       \n",
      "    88     0.721236   8.168178       \n",
      "    89     0.720639   8.181088       \n",
      "    90     0.720058   8.193665       \n",
      "    91     0.719492   8.205898       \n",
      "    92     0.718941   8.217825       \n",
      "    93     0.718403   8.229519       \n",
      "    94     0.71788    8.241074       \n",
      "    95     0.717369   8.252576       \n",
      "    96     0.716871   8.264099       \n",
      "    97     0.716386   8.275684       \n",
      "    98     0.715913   8.287333       \n",
      "    99     0.715451   8.299018       \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[8.299017906188965]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.fit(1*1e-3, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
